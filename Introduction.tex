Automatic speech recognition (ASR) has seen a huge wave of deployment and usage across devices and services in recent years.
Confidence-scores are integral component of ASR, these scores quantify the correctness of recognition results. Historically confidences were used for ASR-enabled devices that are always in an active (continuously) listening mode in an application-constrained grammar. There potential recognitions from side-speech, background noise etc. can trigger unexpected system response. Therefore, it's critical to prevent out-of-grammar (OOG) utterances from being recognized as in-grammar (IG) utterances.
Confidence-scores are typically evaluated for a word as well as for an utterance.

Recently confidence-scores have also been used in other ASR applications in (a) system combination using ROVER, (b) selecting one of client or service recognition in Arbitration, (c) selecting high-quality data for unsupervised model training, (d) Key-word spotting, (e) confidence-normalization etc. While many of the downstream speech applications consume confidence-scores we have seen limited attempts on additionally consuming confidence-features. In this work we present our individual confidence-features, and, discuss the diverse information that they capture. We specifically demonstrate the richness of these features for Arbitration application where we present significant gains in arbitration metric. We also present a DNN speaker adaptation application where we include confidence-scores in adaptation framework.


The confidence classifier is trained to maximally discriminate between correct and incorrect
recognitions. Confidence scores lie in a [0,1] range, we desire higher scores for correct recognitions, and lower for,
(a) incorrect recognitions from in-grammar and, (b) any recognition from out-of-grammar utterances.
The classifiers are trained from a specified set of acoustic model (AM), grammar and speech data, this establishes classifier
profile in terms of CA and FA at different thresholds. We associate an operating threshold with
the classifier and accept recognitions with scores greater than the threshold, upon which we evaluate correct-accept (CA) and false-accept (FA) measures.

We refer \cite{Posen1} for an introduction to our confidence classifier framework. There we
also discussed our predictors confidence classifier approaches.
We refer \cite{CMsurvey_Jiang_SpeechCommunication06, NBest_Wessel_Eurospeech99, MaximumEntropyConfidence_White_ICASSP07,Blatz04confidenceestimation,Rose_UV_1995,Mathan_Rejection_1991,Sukkar_UV_1996}
for a survey of other confidence techniques and specifically \cite{WordLattice_Kemp_Eurospeech97, MaximumEntropyConfidence_White_ICASSP07, Wessel01ney,Chase_wordand,Weintraub97}
for predictors and the classifiers used.

Rest of this work is organized in the following. We explicitly discuss the relevance of 
confidence normalization in Sec.~\ref{sec:ConfNorm}. We present confidence normalization techniques in Secs.~\ref{sec:hist-map}, ~\ref{sec:poly-map},~and \ref{sec:tanh-map} and present 
related experiments and results in Sec.~\ref{sec:results}. Sec.~\ref{sec:conclusion} concludes this study.
